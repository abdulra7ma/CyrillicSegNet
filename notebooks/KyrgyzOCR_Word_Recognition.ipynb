{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import List, Tuple, Optional\n",
    "import logging\n",
    "from torchvision import transforms\n",
    "from IPython.display import display, Image as IPythonImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingModule:\n",
    "    \"\"\"Handles image preprocessing steps.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_image(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply preprocessing steps to the input image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image in BGR format\n",
    "        Returns:\n",
    "            Preprocessed image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply Gaussian Blur\n",
    "            blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            \n",
    "            # Apply adaptive thresholding\n",
    "            binary = cv2.adaptiveThreshold(\n",
    "                blurred, 255, \n",
    "                cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                cv2.THRESH_BINARY_INV, 11, 2\n",
    "            )\n",
    "            \n",
    "            # Apply morphological operations\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "            morphed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "            \n",
    "            return morphed\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationEngine:\n",
    "    \"\"\"Handles letter segmentation from the preprocessed image.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def segment_letters(preprocessed_image: np.ndarray) -> List[Tuple[np.ndarray, Tuple[int, int, int, int]]]:\n",
    "        \"\"\"\n",
    "        Segment individual letters from the preprocessed image.\n",
    "        \n",
    "        Args:\n",
    "            preprocessed_image: Binary preprocessed image\n",
    "        Returns:\n",
    "            List of tuples containing (letter_image, bounding_box)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(\n",
    "                preprocessed_image,\n",
    "                cv2.RETR_EXTERNAL,\n",
    "                cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "            \n",
    "            # Sort contours left to right\n",
    "            contours = sorted(contours, key=lambda x: cv2.boundingRect(x)[0])\n",
    "            \n",
    "            letter_segments = []\n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                \n",
    "                # Filter out noise based on size\n",
    "                if 15 < w < 100 and 15 < h < 100:\n",
    "                    # Extract letter and add padding\n",
    "                    letter_image = preprocessed_image[y:y+h, x:x+w]\n",
    "                    letter_image = cv2.copyMakeBorder(\n",
    "                        letter_image, 10, 10, 10, 10,\n",
    "                        cv2.BORDER_CONSTANT, value=255\n",
    "                    )\n",
    "                    \n",
    "                    # Resize to standard size\n",
    "                    letter_image = cv2.resize(letter_image, (28, 28))\n",
    "                    letter_segments.append((letter_image, (x, y, w, h)))\n",
    "            \n",
    "            return letter_segments\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in segmentation: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/xbp2l_wj5js1yyt_p88w28r00000gn/T/ipykernel_79477/3192331504.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n",
      "ERROR:__main__:Error loading model: 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying network dimensions...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/serialization.py:757\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 154\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlattened size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflattened\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flattened\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mverify_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected size: 12544\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 137\u001b[0m, in \u001b[0;36mverify_dimensions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Verify network dimensions.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVerifying network dimensions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNNRecognitionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[1;32m    138\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Create dummy input\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 14\u001b[0m, in \u001b[0;36mCNNRecognitionModel.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transforms()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 90\u001b[0m, in \u001b[0;36mCNNRecognitionModel.load_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[0;32m---> 90\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Debug information\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/serialization.py:664\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/serialization.py:649\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[0;32m--> 649\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/serialization.py:760\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 760\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/serialization.py:753\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m    747\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m         )\n\u001b[0;32m--> 753\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "class CNNRecognitionModel:\n",
    "    \"\"\"Handles letter recognition using the CNN model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"Initialize the CNN model.\"\"\"\n",
    "        self.classes = [\n",
    "            \"А\", \"Б\", \"В\", \"Г\", \"Д\", \"Е\", \"Ж\", \"З\", \"И\", \"Й\", \"К\", \"Л\", \"М\", \"Н\",\n",
    "            \"О\", \"П\", \"Р\", \"С\", \"Т\", \"У\", \"Ф\", \"Х\", \"Ц\", \"Ч\", \"Ш\", \"Щ\", \"Ъ\", \"Ы\",\n",
    "            \"Ь\", \"Э\", \"Ю\", \"Я\", \"Ң\", \"Ү\", \"Ө\", \"Ё\"\n",
    "        ]\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.transform = self.get_transforms()\n",
    "        self.model = self.load_model(model_path)\n",
    "\n",
    "    def create_model(self) -> nn.Module:\n",
    "        \"\"\"Create the CNN model architecture exactly matching the saved weights.\"\"\"\n",
    "        class KyrgyzCNN(nn.Module):\n",
    "            \"\"\"Base CNN architecture for Kyrgyz character recognition.\"\"\"\n",
    "            def __init__(self, num_classes=36):\n",
    "                super().__init__()\n",
    "                \n",
    "                # Feature extraction layers\n",
    "                self.features = nn.Sequential(\n",
    "                    # First layer: (batch, 3, 112, 112) -> (batch, 16, 112, 112)\n",
    "                    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),  # -> (batch, 16, 56, 56)\n",
    "                    \n",
    "                    # Second layer: (batch, 16, 56, 56) -> (batch, 32, 56, 56)\n",
    "                    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool2d(2),  # -> (batch, 32, 28, 28)\n",
    "                    \n",
    "                    # Third layer: (batch, 32, 28, 28) -> (batch, 16, 28, 28)\n",
    "                    nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True)  # Final size: (batch, 16, 28, 28)\n",
    "                )\n",
    "                \n",
    "                # Calculate flattened size: 16 * 28 * 28 = 12544\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Linear(12544, 2048),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(2048, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(512, num_classes)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Print shapes for debugging\n",
    "                shapes = {}\n",
    "                shapes['input'] = x.shape\n",
    "                \n",
    "                # Ensure input size is correct\n",
    "                if x.size()[-1] != 112 or x.size()[-2] != 112:\n",
    "                    x = F.interpolate(x, size=(112, 112), mode='bilinear', align_corners=False)\n",
    "                    shapes['after_resize'] = x.shape\n",
    "                \n",
    "                # Feature extraction\n",
    "                x = self.features(x)\n",
    "                shapes['after_features'] = x.shape\n",
    "                \n",
    "                # Flatten\n",
    "                x = torch.flatten(x, 1)\n",
    "                shapes['after_flatten'] = x.shape\n",
    "                \n",
    "                # Classification\n",
    "                x = self.classifier(x)\n",
    "                shapes['output'] = x.shape\n",
    "                \n",
    "                return x, shapes\n",
    "        \n",
    "        return KyrgyzCNN(num_classes=len(self.classes))\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        \"\"\"Get image transformations matching the verified input size.\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((112, 112)),  # Changed input size to 112x112\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def load_model(self, model_path: str) -> nn.Module:\n",
    "        \"\"\"Load the trained CNN model with verified architecture.\"\"\"\n",
    "        try:\n",
    "            model = self.create_model()\n",
    "            state_dict = torch.load(model_path, map_location=self.device)\n",
    "            \n",
    "            # Debug information\n",
    "            dummy_input = torch.randn(1, 3, 112, 112)\n",
    "            features_output = model.features(dummy_input)\n",
    "            flattened = features_output.view(1, -1)\n",
    "            logger.info(f\"Input shape: {dummy_input.shape}\")\n",
    "            logger.info(f\"Features output shape: {features_output.shape}\")\n",
    "            logger.info(f\"Flattened size: {flattened.shape[1]}\")\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def recognize_letter(self, letter_image: np.ndarray) -> Tuple[str, float]:\n",
    "        \"\"\"Recognize a single letter using the verified model.\"\"\"\n",
    "        try:\n",
    "            if letter_image.dtype != np.uint8:\n",
    "                letter_image = (letter_image * 255).astype(np.uint8)\n",
    "            \n",
    "            pil_image = Image.fromarray(letter_image).convert('RGB')\n",
    "            tensor_image = self.transform(pil_image)\n",
    "            tensor_image = tensor_image.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(tensor_image)\n",
    "                probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                predicted_letter = self.classes[predicted_idx.item()]\n",
    "                confidence_score = confidence.item()\n",
    "                \n",
    "                return predicted_letter, confidence_score\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in letter recognition: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug function to inspect model state\n",
    "def inspect_model_state(model_path):\n",
    "    \"\"\"\n",
    "    Utility function to inspect the saved model state.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(model_path, map_location='cpu')\n",
    "    print(\"Type of loaded file:\", type(state_dict))\n",
    "    \n",
    "    if isinstance(state_dict, dict):\n",
    "        print(\"\\nKeys in the loaded file:\")\n",
    "        for key in state_dict.keys():\n",
    "            print(f\"- {key}\")\n",
    "            \n",
    "        if 'state_dict' in state_dict:\n",
    "            print(\"\\nKeys in state_dict:\")\n",
    "            for key in state_dict['state_dict'].keys():\n",
    "                print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug utility to print model architecture\n",
    "def print_model_structure(model_path: str):\n",
    "    \"\"\"Print the structure of the saved model.\"\"\"\n",
    "    state_dict = torch.load(model_path, map_location='cpu')\n",
    "    print(\"\\nModel state dict structure:\")\n",
    "    for key, value in state_dict.items():\n",
    "        print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model state dict structure:\n",
      "features.0.weight: torch.Size([16, 3, 3, 3])\n",
      "features.0.bias: torch.Size([16])\n",
      "features.3.weight: torch.Size([32, 16, 3, 3])\n",
      "features.3.bias: torch.Size([32])\n",
      "features.6.weight: torch.Size([64, 32, 3, 3])\n",
      "features.6.bias: torch.Size([64])\n",
      "classifier.0.weight: torch.Size([2048, 12544])\n",
      "classifier.0.bias: torch.Size([2048])\n",
      "classifier.3.weight: torch.Size([512, 2048])\n",
      "classifier.3.bias: torch.Size([512])\n",
      "classifier.6.weight: torch.Size([36, 512])\n",
      "classifier.6.bias: torch.Size([36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/xbp2l_wj5js1yyt_p88w28r00000gn/T/ipykernel_79477/2845816244.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "print_model_structure(\"../results/final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCorrectionModel:\n",
    "    \"\"\"Handles word correction using a Language Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"bert-base-multilingual-cased\"):\n",
    "        \"\"\"\n",
    "        Initialize the LLM correction model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the pretrained model to use\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def correct_word(self, predicted_word: str) -> str:\n",
    "        \"\"\"\n",
    "        Correct the predicted word using the language model.\n",
    "        \n",
    "        Args:\n",
    "            predicted_word: Initial word prediction\n",
    "        Returns:\n",
    "            Corrected word\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # For simplicity, we'll just return the word for now\n",
    "            # In a real implementation, you would:\n",
    "            # 1. Use the LLM to validate the word\n",
    "            # 2. Get potential corrections\n",
    "            # 3. Return the most likely correction\n",
    "            return predicted_word\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in word correction: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KyrgyzWordRecognizer:\n",
    "    \"\"\"Main class that orchestrates the entire recognition process.\"\"\"\n",
    "    \n",
    "    def __init__(self, cnn_model_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the word recognizer with all necessary components.\n",
    "        \n",
    "        Args:\n",
    "            cnn_model_path: Path to the trained CNN model\n",
    "        \"\"\"\n",
    "        self.preprocessor = PreprocessingModule()\n",
    "        self.segmenter = SegmentationEngine()\n",
    "        self.recognizer = CNNRecognitionModel(cnn_model_path)\n",
    "        self.corrector = LLMCorrectionModel()\n",
    "    \n",
    "    def recognize_word(self, image_path: str, visualize: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Recognize a word from an image.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            visualize: Whether to show visualization of the process\n",
    "        Returns:\n",
    "            Recognized word\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"Could not load image\")\n",
    "            \n",
    "            # Preprocess\n",
    "            preprocessed = self.preprocessor.preprocess_image(image)\n",
    "            \n",
    "            # Segment letters\n",
    "            letter_segments = self.segmenter.segment_letters(preprocessed)\n",
    "            \n",
    "            # Recognize letters\n",
    "            predicted_word = \"\"\n",
    "            confidences = []\n",
    "            letter_results = []\n",
    "            \n",
    "            for letter_image, bbox in letter_segments:\n",
    "                letter, confidence = self.recognizer.recognize_letter(letter_image)\n",
    "                predicted_word += letter\n",
    "                confidences.append(confidence)\n",
    "                letter_results.append((letter_image, letter, confidence, bbox))\n",
    "            \n",
    "            # Correct word using LLM\n",
    "            corrected_word = self.corrector.correct_word(predicted_word)\n",
    "            \n",
    "            # Visualize if requested\n",
    "            if visualize:\n",
    "                self._visualize_results(\n",
    "                    image, letter_results, predicted_word, corrected_word\n",
    "                )\n",
    "            \n",
    "            return corrected_word\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in word recognition: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _visualize_results(\n",
    "        self,\n",
    "        original_image: np.ndarray,\n",
    "        letter_results: List[Tuple],\n",
    "        predicted_word: str,\n",
    "        corrected_word: str\n",
    "    ) -> None:\n",
    "        \"\"\"Visualize the recognition results.\"\"\"\n",
    "        # Create a copy for visualization\n",
    "        viz_image = original_image.copy()\n",
    "        \n",
    "        # Draw bounding boxes and predictions\n",
    "        for _, letter, confidence, (x, y, w, h) in letter_results:\n",
    "            cv2.rectangle(viz_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                viz_image,\n",
    "                f\"{letter} ({confidence:.2f})\",\n",
    "                (x, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 255, 0),\n",
    "                1\n",
    "            )\n",
    "        \n",
    "        # Display results\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Original image with annotations\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(viz_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Detected Letters\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Individual letters\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.text(0.5, 0.7, f\"Predicted: {predicted_word}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.text(0.5, 0.3, f\"Corrected: {corrected_word}\",\n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/xbp2l_wj5js1yyt_p88w28r00000gn/T/ipykernel_79477/3192331504.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n",
      "INFO:__main__:Input shape: torch.Size([1, 3, 112, 112])\n",
      "INFO:__main__:Features output shape: torch.Size([1, 16, 28, 28])\n",
      "INFO:__main__:Flattened size: 12544\n",
      "ERROR:__main__:Error loading model: Error(s) in loading state_dict for KyrgyzCNN:\n",
      "\tsize mismatch for features.6.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 32, 3, 3]).\n",
      "\tsize mismatch for features.6.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for KyrgyzCNN:\n\tsize mismatch for features.6.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 32, 3, 3]).\n\tsize mismatch for features.6.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recognizer \u001b[38;5;241m=\u001b[39m \u001b[43mKyrgyzWordRecognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../results/final_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_word(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw/cyrilic_words/combined_word.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecognition result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 13\u001b[0m, in \u001b[0;36mKyrgyzWordRecognizer.__init__\u001b[0;34m(self, cnn_model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;241m=\u001b[39m PreprocessingModule()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmenter \u001b[38;5;241m=\u001b[39m SegmentationEngine()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognizer \u001b[38;5;241m=\u001b[39m \u001b[43mCNNRecognitionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrector \u001b[38;5;241m=\u001b[39m LLMCorrectionModel()\n",
      "Cell \u001b[0;32mIn[69], line 14\u001b[0m, in \u001b[0;36mCNNRecognitionModel.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transforms()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 100\u001b[0m, in \u001b[0;36mCNNRecognitionModel.load_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlattened size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflattened\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for KyrgyzCNN:\n\tsize mismatch for features.6.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 32, 3, 3]).\n\tsize mismatch for features.6.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16])."
     ]
    }
   ],
   "source": [
    "recognizer = KyrgyzWordRecognizer(\"../results/final_model.pth\")\n",
    "result = recognizer.recognize_word(\"../data/raw/cyrilic_words/combined_word.png\")\n",
    "print(f\"Recognition result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "androidmalwaredetector-emBWS5GW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
