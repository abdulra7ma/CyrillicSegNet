{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Step 1: Data Loading and Preprocessing\n",
    "# ----------------------------------------\n",
    "\n",
    "# Kyrgyz Letters Paths\n",
    "kyrgyz_train_path = '../data/raw/handwritten_kyrgyz_letters/train'\n",
    "kyrgyz_test_path = '../data/raw/handwritten_kyrgyz_letters/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyrillic Words Dataset\n",
    "class CyrillicWordsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file, sep='\\t', header=None, names=[\"image\", \"label\"])\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.labels.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied to both datasets (Normalize pixel values between -1 and 1)\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Kyrgyz Dataset Loaders\n",
    "kyrgyz_train_data = torchvision.datasets.ImageFolder(root=kyrgyz_train_path, transform=trans)\n",
    "kyrgyz_test_data = torchvision.datasets.ImageFolder(root=kyrgyz_test_path, transform=trans)\n",
    "\n",
    "# Cyrillic Dataset Loaders\n",
    "cyrillic_train_data = CyrillicWordsDataset(csv_file='../data/raw/cyrilic_words/train.tsv', root_dir='../data/raw/cyrilic_words/train', transform=trans)\n",
    "cyrillic_test_data = CyrillicWordsDataset(csv_file='../data/raw/cyrilic_words/test.tsv', root_dir='../data/raw/cyrilic_words/test', transform=trans)\n",
    "\n",
    "# Combine both datasets\n",
    "combined_train_data = torch.utils.data.ConcatDataset([kyrgyz_train_data, cyrillic_train_data])\n",
    "combined_test_data = torch.utils.data.ConcatDataset([kyrgyz_test_data, cyrillic_test_data])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(combined_train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(combined_test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Step 2: Model Definition (CNN)\n",
    "# ----------------------------------------\n",
    "\n",
    "# Define a Convolutional Neural Network model for both Kyrgyz letters and Cyrillic words\n",
    "class KyrgyzCyrillicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KyrgyzCyrillicNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3),  # Conv Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Pooling Layer 1\n",
    "            nn.Conv2d(16, 32, kernel_size=3),  # Conv Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Pooling Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3),  # Conv Layer 3\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)  # Pooling Layer 3\n",
    "        )\n",
    "        self.flatten = nn.Flatten()  # Flatten the output for the fully connected layers\n",
    "\n",
    "        # Fully connected layers (final classification)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64*15*15, 2048),  # Dense Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),  # Dropout for regularization\n",
    "            nn.Linear(2048, 512),  # Dense Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 1036)  # Output layer (1036 classes: 36 Kyrgyz letters + ~1000 Cyrillic words)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Pass through convolutional layers\n",
    "        x = self.flatten(x)  # Flatten output\n",
    "        logits = self.classifier(x)  # Pass through fully connected layers\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KyrgyzCyrillicNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=14400, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=1036, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and move to the correct device (CPU/GPU)\n",
    "model = KyrgyzCyrillicNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        model (nn.Module): The neural network model.\n",
    "        loss_fn (nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss and top-1 accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Iterate over batches of data\n",
    "    for batch_idx, (imgs, labels) in enumerate(dataloader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass: compute model predictions\n",
    "        pred = model(imgs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # Backward pass: compute gradients and update parameters\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Accumulate loss and calculate top-1 accuracy\n",
    "        total_loss += loss.item()\n",
    "        predicted_1 = pred.argmax(1)\n",
    "        top1_acc += (predicted_1 == labels).float().sum().item()\n",
    "\n",
    "        # Log progress every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            current = batch_idx * len(imgs)\n",
    "            print(f\"[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return total_loss / size, top1_acc / size\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"Evaluate the model on validation or test data.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for validation or test data.\n",
    "        model (nn.Module): The neural network model.\n",
    "        loss_fn (nn.Module): The loss function.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss and top-1 accuracy for the validation/test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "\n",
    "    # No gradient computation needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass: compute model predictions\n",
    "            pred = model(imgs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(pred, labels)\n",
    "\n",
    "            # Accumulate loss and calculate top-1 accuracy\n",
    "            total_loss += loss.item()\n",
    "            predicted_1 = pred.argmax(1)\n",
    "            top1_acc += (predicted_1 == labels).float().sum().item()\n",
    "\n",
    "    return total_loss / size, top1_acc / size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a results folder if it doesn't exist\n",
    "os.makedirs('../results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Step 4: Hyperparameters and Training Loop\n",
    "# ----------------------------------------\n",
    "\n",
    "# Define the loss function\n",
    "# CrossEntropyLoss is used here as it's suitable for classification problems.\n",
    "# It combines LogSoftmax and Negative Log Likelihood Loss, making it ideal for multi-class classification.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "# The Adam optimizer is used because it combines the advantages of two popular optimizers:\n",
    "# AdaGrad (good at handling sparse gradients) and RMSProp (good at handling non-stationary objectives).\n",
    "# It's adaptive and works well with a large variety of deep learning tasks.\n",
    "# The learning rate (lr) is initially set to 1e-3, which is a common starting value that often works well.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "# The ReduceLROnPlateau scheduler reduces the learning rate when the validation loss plateaus.\n",
    "# mode='min' means that we want the validation loss to minimize.\n",
    "# patience=3 means it will wait for 3 epochs without improvement before reducing the learning rate.\n",
    "# factor=0.5 reduces the learning rate by half.\n",
    "# verbose=True prints a message whenever the learning rate is reduced.\n",
    "# This helps in fine-tuning the model as it converges, allowing it to take smaller steps to refine training.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping configuration\n",
    "# Early stopping helps prevent overfitting by halting training once there is no improvement in validation loss.\n",
    "# patience=5 means training will stop if there is no improvement for 5 consecutive epochs.\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss to infinity\n",
    "# This means that initially any computed validation loss will be lower than this value.\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for how many epochs in a row validation loss has not improved.\n",
    "# If it reaches the value of 'patience', training will stop.\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Target accuracy for stopping training early if reached\n",
    "target_accuracy = 0.95  # Stop training if validation accuracy reaches or exceeds 95%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to track results of training and validation across epochs\n",
    "# This will store the losses and accuracies for later analysis.\n",
    "res = {\n",
    "    \"train_loss\": [],       # To store training loss for each epoch\n",
    "    \"train_top1_acc\": [],   # To store training top-1 accuracy for each epoch\n",
    "    \"val_loss\": [],         # To store validation loss for each epoch\n",
    "    \"val_top1_acc\": [],     # To store validation top-1 accuracy for each epoch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs\n",
    "# The model will train for a maximum of 50 epochs, but early stopping may stop it sooner.\n",
    "# The number of epochs can vary depending on the size and complexity of the dataset and model.\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the best model\n",
    "# This is used to store the model with the lowest validation loss during training.\n",
    "best_model_path = '../results/models/best_model.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 72, 202] at entry 0 and [3, 54, 162] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train phase: Train the model using the training dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_loss, train_top1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Validation phase: Evaluate the model using the validation dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m val_loss, val_top1 \u001b[38;5;241m=\u001b[39m test(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Iterate over batches of data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     20\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Forward pass: compute model predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/androidmalwaredetector-emBWS5GW-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 72, 202] at entry 0 and [3, 54, 162] at entry 1"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n{'-'*40}\")\n",
    "    \n",
    "    # Train phase: Train the model using the training dataset\n",
    "    train_loss, train_top1 = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # Validation phase: Evaluate the model using the validation dataset\n",
    "    val_loss, val_top1 = test(test_dataloader, model, loss_fn)\n",
    "    \n",
    "    # Save the results for the current epoch\n",
    "    res['train_loss'].append(train_loss)\n",
    "    res['train_top1_acc'].append(train_top1)\n",
    "    res['val_loss'].append(val_loss)\n",
    "    res['val_top1_acc'].append(val_top1)\n",
    "\n",
    "    # Print epoch summary: Show the training and validation loss and accuracy\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Top-1 Accuracy: {train_top1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Top-1 Accuracy: {val_top1:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping and model checkpointing\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        # Save the model as the best model so far\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"No improvement in validation loss for {early_stopping_counter} consecutive epochs.\")\n",
    "    \n",
    "    # Check if early stopping is needed based on patience\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered due to no improvement. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    # Check if the target accuracy is reached to stop training\n",
    "    if val_top1 >= target_accuracy:\n",
    "        print(f\"Target accuracy of {target_accuracy*100:.2f}% reached. Stopping training.\")\n",
    "        break\n",
    "\n",
    "# Print final training status\n",
    "print('Training Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "androidmalwaredetector-emBWS5GW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
